Title: Bericht MLUGS Treffen im Juni 2016
Date: 2016-06-22 10:00

## Protokoll

### Vorstellungsrunde

* Andreas; ax-semantics; Software-Entwickler
* Frank; IoT/Maschinenbau-Software-Entwicklung
* David; arbeitet sich in ML rein; arbeitet Bücher durch
* Viktor; Software-Architekt bei softwareinmotion; embedded/web. inzw. ML; verwendet F#
* Michael; Kaufland; Datamining
* Armin; Kaufland; Data-Warehousing
* Wolfgang; Kaufland; Innovation-Management, auch Kaufland-IT macht cooles Zeug
* Robin; Technische Biologie-Student; shack
* Stefan; hat eine Firma; Java/ERP

+1 Nachzügler


### Dr. Frank Gerhardt - Machine Learning with PySpark

wurde schon als Workshop auf der PyData in Berlin gehalten.

[https://hub.docker.com/r/gerhardt/pyspark-workshop/](https://hub.docker.com/r/gerhardt/pyspark-workshop/)

Die Idee hinter PySpark: Apache Spark ist ein Framework um Kollektions beliebiger Größe verarbeiten zu können. Massiv parallel.

Macht quasi map/reduce.

Frank würde eher Cassandra verwenden. Und nicht Hadoop.

Spark arbeitet komplett im RAM. Und verwendet eine Datenbank/Festplatte.

Hat sehr viele ML-Funktionen fertig implementiert dabei.

Idee: mit spark macht man aus vielen servern nach aussen hin einen Server.

Integrierte Fehlertoleranz. Wenn ein Knoten ausfällt, werden die Jobs neu verteilt. 

PySpark redet über socket/pipes mit Spark/JVM.

DataFrame als basis der Daten hat ein Schema. Das Schema wird abgeleitet aus den Daten. Das Schema sollte sich dann nicht mehr ändern.

PySpark hängt immer etwas hinterher in der Entwicklung, weil eben erst Java und Scala entwickelt werden.

DataFrame ist in Scala, Python, R und SQL gleich schnell.

Zusammenfassung: Spark kann Collections berechnen und ist dabei nicht durch den RAM beschränkt.

Todo jetzt: Datensammeln!! Später kann man dann modelle darauf rechnen. Wenn man keine Daten hat, dann kann man kein ML machen!

### Diskussion

Wie beeinflusst ML Nutzer und wie verändern Nutzer ihr Verhalten, um den richtigen Effekt zu bekommen (und "tricksen" damit den Algorithmus aus).

Privacydiskussion. Wenn der Laden den Nutzer auf Basis der Mac-Adresse im Wifi trackt.

#### alternativer Mitschrieb zum Vortrag

Spark

* große Datenmengen, die nicht nach oben begrenzt sind (nur Plattenplatz/memory ist limitierend)
* Streamverarbeitung: 
    
findspark # Hilft beim auffinden

~~~
sc=pysparc.SparkContext()
sc.parallelize(range(1,1000),4).map(lambda x: 1/x)
~~~

Cluster aus NUCs, gemeinsame Nutzung von Hauptspeicher, HD etc. 

##### What is spark

* implementiert in Java/

|         |                         |                      |                        |
|---------|-------------------------|----------------------|------------------------|
|Spark SQL|Sparc streaming real-time|MLlib machine learning| GraphX graph processing|
|sparc    |                         |                      |                        |
|Scheduler| YARN                    |                 Mesos|                        |


MLlib: machine learning lib
pyspark.ml

    Spark driver
        -> 
        Cluster master
            -> Cluster worker

###### main concept: RDD
    
* RDD: resilient distributed dataset
* resilient: recover from failures
* distributed
* dataset: can be large (much larger than RAM)

RDD metadata

* partitions (eher viele, 10000)
* dependencies
* compute function
* preferred locations
* partitioner

RDD creation from
* persistent storage: files, HDFS, Cassandra, HBAse
* python collection
* ...

RDD caching

* memory
* mem+disk
* disk (seldom used)

~~~
rdd1=sc.range(1,12345)
rdd2=rdd1.map(lambda x: x+1)
rdd2.collect()
# rdd2.cache() # otherwise result needs to be recomputed if needed later
~~~

transformations and actions

* Aufbau eines Execute-Graphen

Demo word-count im jupyter notebook

##### pysparc

Cluster

* Spark Worker on JVM
    * <- Pipe -> Python  
* Spark Worker on JVM
    * <- Pipe -> Python  

Local

* Py4J 
    * Spark Context
    -> Local file system
    -> ...

Data frame

* DF has schema
* not as lazy as RDDs are

Data Sets

* wegen fehlender Typisierung: not available yet

##### Pointer

* Advanced Analytics with Spark: Patterns for Learning from Data at Scale http://shop.oreilly.com/product/0636920035091.do
* https://github.com/gerhardt-io


### Lightningtalks

#### Andreas Madsack - TensorFlow auf dem Raspberry PI 3

[https://github.com/mlugs/jupyter/blob/master/notebook/Lightning%20Talk%20-%20Tensorflow%20on%20RaspberryPi%203.ipynb](https://github.com/mlugs/jupyter/blob/master/notebook/Lightning%20Talk%20-%20Tensorflow%20on%20RaspberryPi%203.ipynb)

Ähnlich sinnvolle Anwendung von ML: http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/


#### Andreas Madsack - Parsey McParseface and SyntaxNet

[https://github.com/mlugs/jupyter/blob/master/notebook/Lightning%20Talk%20-%20Parsey%20McParseface%20and%20SyntaxNet.ipynb](https://github.com/mlugs/jupyter/blob/master/notebook/Lightning%20Talk%20-%20Parsey%20McParseface%20and%20SyntaxNet.ipynb)


#### Martin Weis - Data Mining

Herbologie an der Uni-Hohenheim.

Schnelldurchlauf über Data-Mining.

hilfreich: deviation plot (mittelwert/std-abweichung)

alternative software: [http://community.pentaho.com/](http://community.pentaho.com/)


### next

wir wollen business-usecases zusammentragen und diskutieren.

nächster Termin ist Dienstag, der 19. Juli 2016!
